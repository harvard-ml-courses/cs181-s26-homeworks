\documentclass{../harvardml}

% Authors: 
% Edited by: Mark Goldstein + others (jan 2018)
% Edited by: Amir Shanehsazzadeh, Andrew Kim, Nari Johnson (Jan 2021)
% Edited by: Max Guo, Raphael Pellegrin, Katherine Tian (Jan 2022)
% Edited once more by: William Tong (Jan 2023) + Skyler Wu (Jan 2023)
% Edited once more by: Jeffrey Xu (Jan 2024) + Gabriel Sun (Jan 2024)
% Edited once more by: Gabriel Sun (Jan 2025)
% Edited once more by: Russell Li (Jan 2026)

% Adapted from CS281 Fall 2019 section 0 notes

% This tex file relies on
% the presence of two files:
% harvardml.cls and common.sty

\course{CS1810-s26}
\assignment{Homework \#0}
\duedate{February 2, 2026 at 11:59 PM}

\usepackage{url}
\usepackage{float}
\usepackage{amsfonts, amsmath, amsthm}
\usepackage{listings}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{etoolbox}
\usepackage{bm}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\theoremstyle{plain}
\usepackage[textsize=tiny]{todonotes}
\usepackage{../common}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Solution environment
\usepackage{xcolor}
\usepackage{comment}
\newenvironment{solution}
  {\color{blue}\section*{Solution}}
{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}



\noindent Welcome to CS1810! The purpose of this assignment is to help assess your readiness for this course.  It will be graded for completeness and effort.  \textbf{Areas of this assignment that are difficult are an indication of areas in which \emph{you} need to self-study. If you find you are struggling with many of these questions, it might be prudent to postpone taking this course until after you have mastered the necessary prerequisites.  \emph{During the term, the staff will be prioritizing support for new material taught in CS1810 over teaching prerequisites.}  If you are unsure about your readiness, please contact the head TFs for advice.}


\begin{enumerate}
    \item Please type your solutions after the corresponding problems using this \LaTeX\ template, and start each problem on a new page.
    \item Please submit the \textbf{writeup PDF to the Gradescope assignment `HW0'}. Remember to assign pages for each question.
    \item Please submit your \textbf{\LaTeX\ file and code files (i.e., anything ending in \texttt{.py}, \texttt{.ipynb}, or \texttt{.tex}) to the Gradescope assignment `HW0 - Supplemental'}. 
\end{enumerate}

\newpage
\begin{problem}[Modeling Linear Trends - Linear Algebra Review]
In this class, we will be exploring the question of ``how do we model the trend in a dataset" under different guises. In this problem, we will explore the algebra of modeling a linear trend in data. We call the process of finding a model that capture the trend in the data, ``fitting the model."\\

\noindent \textbf{Learning Goals:} In this problem, you will practice translating machine learning goals (``modeling trends in data") into mathematical formalism using linear algebra. You will explore how the right mathematical formalization can help us express our modeling ideas unambiguously and provide ways for us to analyze different pathways to meeting our machine learning goals.\\

\noindent Let's consider a dataset consisting of two points $\mathcal{D} = \{(x_1, y_1), (x_2, y_2)\}$, where $x_n, y_n$ are scalars for $n=1, 2$. Recall that the equation of a line in 2-dimensions can be written: $y = w_0 + w_1x$. 
\begin{enumerate}
    \item Write a system of linear equations determining the coefficients $w_0, w_1$ of the line passing through the points in our dataset $\mathcal{D}$ and analytically solve for $w_0, w_1$ by solving this system of linear equations (i.e., using substitution). Please show your work.
    \item Write the above system of linear equations in matrix notation, so that you have a matrix equation of the form $\mathbf{y} = \mathbf{X}\mathbf{w}$, where $\mathbf{y}, \mathbf{w} \in \mathbb{R}^2$ and $\mathbf{X} \in \mathbb{R}^{2\times 2}$. For full credit, it suffices to write out what $\mathbf{X}$, $\mathbf{y}$, and $\mathbf{w}$ should look like in terms of $x_1$, $x_2$, $y_1$, $y_2$, $w_0$, $w_1$, and any other necessary constants. Please show your reasoning and supporting intermediate steps.
    \item Using properties of matrices, characterize exactly when an unique solution for  $\mathbf{w}=\left(w_0 \; w_1 \right)^{T}$ exists. In other words, what must be true about your dataset in order for there to be a unique solution for $\mathbf{w}$? When the solution for $\mathbf{w}$ exists (and is unique), write out, as a matrix expression, its analytical form (i.e., write $\mathbf{w}$ in terms of $\mathbf{X}$ and $\mathbf{y}$).
    
    Hint: What special property must our $\mathbf{X}$ matrix possess? What must be true about our data points in $\mathcal{D}$ for this special property to hold?
    \item Compute $\mathbf{w}$ by hand via your matrix expression in (3) and compare it with your solution in (1). Do your final answers match? What is one advantage for phrasing the problem of fitting the model in terms of matrix notation? 
    \item In real-life, we often work with datasets that consist of hundreds, if not millions, of points. In such cases, does our analytical expression for $\mathbf{w}$ that we derived in (3) apply immediately to the case when $\mathcal{D}$ consists of more than two points? Why or why not?
    \item Using Python, construct matrix $\mathbf{X}$ and vector $\mathbf{y}$ corresponding to a dataset $\mathcal{D} = \{(x_1, y_1), (x_2, y_2)\}$. Compute $\mathbf{w}$ using numerical values of your choice with $x_1 \neq x_2$. Your code should reflect the matrix formulation derived above.

\end{enumerate}
    
\end{problem}

\newpage


\begin{solution}

1. Given two data points $D=\{(x_1,y_1),(x_2,y_2)\}$ and a line \(y = w_0 + w_1 x,\) the coefficients $(w_0,w_1)$ must satisfy the system
\[
\begin{cases}
y_1 = w_0 + w_1 x_1,\\
y_2 = w_0 + w_1 x_2.
\end{cases}
\]
Subtract the first equation from the second: \(y_2 - y_1 = w_1(x_2 - x_1).\) If $x_1 \neq x_2$, then
\[
w_1 = \frac{y_2-y_1}{x_2-x_1}.
\]
Substitute back into $y_1 = w_0 + w_1 x_1$:
\[
w_0 = y_1 - w_1 x_1
     = y_1 - x_1\frac{y_2-y_1}{x_2-x_1}.
\]
Equivalently, we can also compute for \(w_0 = y_2 - x_2\frac{y_2-y_1}{x_2-x_1}.\) 
\\
\\
2. From
\[
\begin{cases}
y_1 = w_0 + w_1 x_1,\\
y_2 = w_0 + w_1 x_2,
\end{cases}
\]
we can rewrite each equation as an inner product:
\[
y_n = \begin{bmatrix}1 & x_n\end{bmatrix}
\begin{bmatrix}w_0\\ w_1\end{bmatrix},
\qquad n=1,2.
\]
Stacking the two equations gives
\[
\begin{bmatrix}y_1\\ y_2\end{bmatrix} = 
\begin{bmatrix}
1 & x_1\\
1 & x_2
\end{bmatrix}
\begin{bmatrix}w_0\\ w_1\end{bmatrix}.
\]
Therefore, in the form $\mathbf{y}=\mathbf{X}\mathbf{w}$,
\[
\mathbf{y}=\begin{bmatrix}y_1\\ y_2\end{bmatrix}\in\mathbb{R}^2,\quad
\mathbf{w}=\begin{bmatrix}w_0\\ w_1\end{bmatrix}\in\mathbb{R}^2,\quad
\mathbf{X}=\begin{bmatrix}
1 & x_1\\
1 & x_2
\end{bmatrix}\in\mathbb{R}^{2\times 2}.
\]
\\
\\
3. We have the linear system
\[
\mathbf{y}=\mathbf{X}\mathbf{w},\qquad where\
\mathbf{X}=\begin{bmatrix}1 & x_1\\ 1 & x_2\end{bmatrix},\ 
\mathbf{y}=\begin{bmatrix}y_1\\ y_2\end{bmatrix},\ 
\mathbf{w}=\begin{bmatrix}w_0\\ w_1\end{bmatrix}.
\]
An unique solution $\mathbf{w}$ exists iff $\mathbf{X}$ is invertible; equivalently, $\det(\mathbf{X})\neq 0$ or $\mathrm{rank}(\mathbf{X})=2$.
Compute
\[
\det(\mathbf{X})=
\begin{vmatrix}
1 & x_1\\
1 & x_2
\end{vmatrix}
= x_2-x_1.
\]
Hence $\mathbf{X}$ is invertible iff $x_1\neq x_2$. In terms of the dataset, we need the two points to have distinct $x$-coordinates; otherwise the two rows of $\mathbf{X}$ are identical and the system cannot determine a unique line. When $x_1\neq x_2$, the unique solution is
\[
\mathbf{w}=\mathbf{X}^{-1}\mathbf{y}.
\]
\\
\\
4. Assuming $x_1\neq x_2$, so $\mathbf{X}$ is invertible. From (3),
\[
\mathbf{w}=\mathbf{X}^{-1}\mathbf{y},\qquad 
\mathbf{X}=\begin{bmatrix}1 & x_1\\ 1 & x_2\end{bmatrix},\ 
\mathbf{y}=\begin{bmatrix}y_1\\ y_2\end{bmatrix}.
\]
Compute $\mathbf{X}^{-1}$ for a $2\times 2$ matrix:
\[
\mathbf{X}^{-1}=\frac{1}{\det(\mathbf{X})}
\begin{bmatrix}
x_2 & -x_1\\
-1 & 1
\end{bmatrix}
=\frac{1}{x_2-x_1}
\begin{bmatrix}
x_2 & -x_1\\
-1 & 1
\end{bmatrix}.
\]
Thus
\[
\mathbf{w}
=\frac{1}{x_2-x_1}
\begin{bmatrix}
x_2 & -x_1\\
-1 & 1
\end{bmatrix}
\begin{bmatrix}y_1\\y_2\end{bmatrix}
=\frac{1}{x_2-x_1}
\begin{bmatrix}
x_2y_1-x_1y_2\\
-y_1+y_2
\end{bmatrix}.
\]
Therefore
\[
w_1=\frac{y_2-y_1}{x_2-x_1},\qquad
w_0=\frac{x_2y_1-x_1y_2}{x_2-x_1}.
\]
These match the solution from (1), since
\[
w_0=y_1-x_1\frac{y_2-y_1}{x_2-x_1}
=\frac{y_1(x_2-x_1)-x_1(y_2-y_1)}{x_2-x_1}
=\frac{x_2y_1-x_1y_2}{x_2-x_1}.
\]
One advantage of using matrix notation is that, even for many data points, we can still write $\mathbf{y}=\mathbf{X}\mathbf{w}$ and use standard linear algebra methods instead of doing case-by-case substitution.
\\
\\
5. When $D$ contains more than two points, the matrix $\mathbf{X}\in\mathbb{R}^{n\times 2}$ with $n>2$ is no longer square, so $\mathbf{X}^{-1}$ does not exist. So the expression \(\mathbf{w}=\mathbf{X}^{-1}\mathbf{y}\)  does not apply directly. In general, with more equations than unknowns, the system $\mathbf{y}=\mathbf{X}\mathbf{w}$ is overdetermined and typically has no exact solution. Instead, one solves the least squares problem
\[
\min_{\mathbf{w}}\|\mathbf{y}-\mathbf{X}\mathbf{w}\|^2,
\]
whose solution is
\[
\mathbf{w}=(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}.
\]
Hence the analytical inverse formula only applies in the square matrix case. With many data points, we must use the normal equation or other optimization methods.
\\
\\
6. \textbf{Code:}
\begin{verbatim}
x1, y1 = 1, 2
x2, y2 = 3, 6

X = np.array([[1, x1], [1, x2]])
y = np.array([y1, y2])

w = np.linalg.solve(X, y)
w0, w1 = w

print("X =\n", X)
print("y =", y.tolist())
print("w =", w.tolist())

print("check y1:", w0 + w1*x1)
print("check y2:", w0 + w1*x2)
\end{verbatim}
\textbf{Output:}
\begin{verbatim}
X =
 [[1 1]
 [1 3]]
y = [2, 6]
w = [0.0, 2.0]
check y1: 2.0
check y2: 6.0
\end{verbatim}

\end{solution}

\color{black}
\newpage


\begin{problem}[Optimizing Objectives - Calculus Review]
In this class, we will write real-life goals we want our model to achieve into a mathematical expression and then find the optimal settings of the model that achieves these goals. The formal framework we will employ is that of mathematical optimization. Although the mathematics of optimization can be quite complex and deep, we have all encountered basic optimization problems in our first calculus class!\\

\noindent \textbf{Learning Goals:} In this problem, we will explore how to formalize real-life goals as mathematical optimization problems. We will also investigate under what conditions these optimization problems have solutions.\\

\noindent In her most recent work-from-home shopping spree, Nari decided to buy several house plants. \textit{Her goal is to make them to grow as tall as possible.} After perusing the internet, Nari learns that the height $y$ in mm of her Weeping Fig plant can be directly modeled as a function of the oz of water $x$ she gives it each week:
$$y = - 3x^2 + 72x + 70.$$
\begin{enumerate}
    \item First, plot the height function. What does the plot tell you about the existence and uniqueness of a maximum plant height? Next, support your claim solely based on the form of the function.

    \item Use calculus to find how many ounces of water per week Nari should give to her plant in order to maximize its height. With this much water, how tall will her plant grow?

\end{enumerate}
Now suppose that Nari want to optimize both the amount of water $x_1$ (in oz) \textit{and} the amount of direct sunlight $x_2$ (in hours) to provide for her plants. After extensive research, she decided that the height $y$ (in mm) of her plants can be modeled as a two variable function:

$$y = f(x_1, x_2) = \exp\left(-(x_1 - 2)^2 - (x_2 - 1)^2 \right)$$
\begin{enumerate}
    \setcounter{enumi}{2}
    \item Using \texttt{matplotlib}, visualize in 3D the height function as a function of $x_1$ and $x_2$ using the \texttt{plot\_surface} utility for $(x_1, x_2) \in (0, 6) \times (0, 6)$. Then, determine the values of $x_1$ and $x_2$ that maximize plant height. Do these yield a global maximum? 
    
    Hint: You don't need to take any derivatives here; reasoning about the form of $f(x_1, x_2)$ suffices.
\end{enumerate}
\end{problem}

\newpage


\begin{solution}

1. \textbf{Code and Plot:}
\begin{verbatim}
x = np.linspace(-5, 30, 400)
y = -3*x**2 + 72*x + 70
plt.plot(x, y)
plt.xlabel("x")
plt.ylabel("y")
plt.title("y = -3x^2 + 72x + 70")
\end{verbatim}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig 2.1.png}
\end{figure}
The plot shows a downward-opening parabola with a single peak, indicating that a maximum height exists and is unique. This follows from the functional form: since the coefficient of $x^2$ is negative, the quadratic opens downward and therefore attains a unique global maximum at its vertex.
\\
\\
2. To maximize \(y(x)=-3x^2+72x+70,\) we take the derivative:
\[
\frac{dy}{dx}=-6x+72.
\]
\[
-6x+72=0 \quad \Rightarrow \quad x=12.
\]
Since the quadratic opens downward ($-3<0$), this critical point corresponds to a maximum. And the maximum height is
\[
y(12)=-3(12)^2+72(12)+70
= -432+864+70
=502.
\]
Thus, Nari should give the plant $12$ ounces of water per week, and the plant will grow to a height of $502$ mm.
\\
\\
3. \textbf{Code and Plot:}
\begin{verbatim}
x1 = np.linspace(0, 6, 200)
x2 = np.linspace(0, 6, 200)
X1, X2 = np.meshgrid(x1, x2)

Y = np.exp(-((X1 - 2)**2 + (X2 - 1)**2))

fig = plt.figure()
ax = fig.add_subplot(111, projection="3d")
ax.plot_surface(X1, X2, Y)
ax.set_xlabel("x1 (oz water)")
ax.set_ylabel("x2 (hours sunlight)")
ax.set_zlabel("y (height)")
ax.set_title(r"$y=\exp(-((x_1-2)^2+(x_2-1)^2))$")
\end{verbatim}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig 2.3.png}
\end{figure}
Since $(x_1-2)^2\ge 0$ and $(x_2-1)^2\ge 0$, we have
\[
-(x_1-2)^2-(x_2-1)^2 \le 0
\quad\Rightarrow\quad
f(x_1,x_2)\le e^{0}=1,
\]
with equality iff $(x_1-2)^2=(x_2-1)^2=0$, i.e., $(x_1,x_2)=(2,1)$. Thus the maximizer is $(x_1,x_2)=(2,1)$ and the maximum value is $f(2,1)=1$. Which is the global maximum.

\end{solution}

\newpage 


\begin{problem}[Reasoning about Randomness - Probability and Statistics Review]
In this class, one of our main focuses is to model the unexpected variations in real-life phenomena using the formalism of random variables. In this problem, we will use random variables to model how much time it takes an USPS package processing system to process packages that arrive in a day.\\

\noindent \textbf{Learning Goals:} In this problem, you will analyze random variables and their distributions both analytically and computationally. You will also practice drawing connections between said analytical and computational conclusions.\\

\noindent Consider the following model for each package that arrives at the US Postal Service (USPS):
\begin{itemize}
    \item Every package has a random size $S$ (measured in $in^3$) and weight $W$ (measured in pounds), with joint distribution
    $$(S, W)^{T} \sim \mathcal{N}\left( \boldsymbol{\mu}, \boldsymbol{\Sigma}\right) \text{, with } \boldsymbol{\mu} = \begin{bmatrix} 120 \\ 4 \end{bmatrix} \text{ and } \boldsymbol{\Sigma} = \begin{bmatrix} 1.5 & 1 \\ 1 & 1.5 \end{bmatrix}.$$ 
    \item The size and weight of each package is independent of those of all the other packages.
    \item Processing time $T$ (in seconds) for each package is given by $T = 60 + 0.6 W + 0.2 S + \epsilon$, where $\epsilon$ is an independent random noise variable with Gaussian distribution $\epsilon \sim \mathcal{N}(0, 5)$.
\end{itemize}

\begin{enumerate}
    \item Perform the following tasks:
    \begin{enumerate}
        \item Give one reason for why the Gaussian distribution may not be appropriate for modeling the size and weight of packages.
        
        \item Empirically estimate the most likely combination of size and weight of a package by sampling 500 times from the joint distribution of $S$ and $W$ and generating a bivariate histogram of your $S$ and $W$ samples. A visual inspection is sufficient -- you do not need to be incredibly precise.  How close are these empirical values to the theoretical expected size and expected weight of a package, according to the given Bivariate Gaussian distribution?

        Hint: For this part, you may find the \texttt{multivariate\_normal} module from \texttt{scipy.stats} especially helpful. You may also find the \texttt{seaborn.histplot} function quite helpful. 
    \end{enumerate}
    
    \item For 1001 evenly-spaced values of $W$ between $0$ and $10$, plot $W$ versus the joint Bivariate Gaussian PDF $p(W, S)$ with $S$ fixed at $S=118$. Repeat this procedure for $S$ fixed at $S=122$. Comparing these two PDF plots, what can you say about the correlation of random variables $S$ and $W$? 
    
    \item Because $T$ is a linear combination of random variables, it itself is a random variable. Using properties of expectations and variance, please compute $\mathbb{E}(T)$ and $\mathrm{Var}(T)$ analytically.
    
    \item Define $N$ to be the number of packages that arrive today, and suppose that packages that weigh less than 4 pounds are considered fragile. Conditional on $N=n$, what is the name and PMF of the distribution of the number of fragile packages that arrive today?
    
    \item Now suppose that $N = \sum_{h=1}^{24}P_h$, where the $P_h$ are independent and identically distributed as $\text{Pois}(\lambda=3)$. Then define $T^* = \sum_{i=1}^{N} T_i$ as the  \textit{total} amount of time it takes to process \textit{all} these packages, where $T_i$ follows the distribution of $T$ that we previously defined for each package.
    \begin{enumerate}
        \item Write a function to simulate draws from the distribution of $T^{*}$. 
        \item Using your function, empirically estimate the mean and standard deviation of $T^{*}$ by generating $1000$ samples from the distribution of $T^{*}$.
    \end{enumerate}
\end{enumerate}
\end{problem}

\newpage

\begin{solution}

1. (a). A Gaussian model assigns positive probability to impossible values such as negative size or weight, namely
\[
\mathbb{P}(S<0)>0 \quad \text{and} \quad \mathbb{P}(W<0)>0,
\]
but physical packages must satisfy \(S\ge 0\) and \(W\ge 0\).
\\
\\
1.(b). We draw \(500\) samples from the joint Gaussian distribution
\[
(S,W)^T \sim \mathcal{N}\!\left(
\begin{bmatrix}120\\4\end{bmatrix},
\begin{bmatrix}1.5 & 1\\ 1 & 1.5\end{bmatrix}
\right),
\]
and generate a bivariate histogram to visualize the joint density. \textbf{Code:}
\begin{verbatim}
mu = np.array([120, 4])
Sigma = np.array([[1.5, 1],[1, 1.5]])

samples = mvn(mean=mu, cov=Sigma).rvs(size=500)

S = samples[:, 0]
W = samples[:, 1]

sns.histplot(x=S, y=W, bins=30, cbar=True)
plt.xlabel("S (in^3)")
plt.ylabel("W (lb)")
plt.title("Bivariate Histogram of (S, W)")

print("Empirical mean of S:", S.mean())
print("Empirical mean of W:", W.mean())
\end{verbatim}
\textbf{Output:}
\begin{verbatim}
Empirical mean of S: 120.01055322576612
Empirical mean of W: 4.041579855230118
\end{verbatim}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig 3.1.png}
\end{figure}
The empirical means are \(\hat{\mathbb{E}}[S] = 120.01,\
\hat{\mathbb{E}}[W] = 4.04,\) which are very close to the theoretical values\(\mathbb{E}[S] = 120,\ \mathbb{E}[W] = 4.\)Thus, the bivariate histogram peaks near \((120,4)\), confirming the expected most likely size and weight.
\\
\\
2. \textbf{Code and Plot:}
\begin{verbatim}
mu = np.array([120, 4])
Sigma = np.array([[1.5, 1], [1, 1.5]])

rv = mvn(mean=mu, cov=Sigma)

W_vals = np.linspace(0, 10, 1001)

S1 = 118
S2 = 122

points_S1 = np.column_stack((np.full_like(W_vals, S1), W_vals))
points_S2 = np.column_stack((np.full_like(W_vals, S2), W_vals))

pdf_S1 = rv.pdf(points_S1)
pdf_S2 = rv.pdf(points_S2)

plt.plot(W_vals, pdf_S1, label="S = 118")
plt.plot(W_vals, pdf_S2, label="S = 122")

plt.xlabel("W")
plt.ylabel("p(W, S)")
plt.title("Joint PDF slices at S = 118 and S = 122")
\end{verbatim}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig 3.2.png}
\end{figure}
From the plots, when \(S=118\), the joint density \(p(W,S)\) attains its peak at a smaller value of \(W\).  When \(S=122\), the peak shifts to a larger value of \(W\).  This indicates that larger values of \(S\) tend to be associated with larger values of \(W\), so \(S\) and \(W\) are positively correlated. Moreover, from the covariance matrix
\[
\Sigma=\begin{bmatrix}1.5 & 1\\ 1 & 1.5\end{bmatrix},
\]
the correlation coefficient is
\[
\mathrm{Corr}(S,W)=\frac{\mathrm{Cov}(S,W)}{\sqrt{\mathrm{Var}(S)\mathrm{Var}(W)}}
=\frac{1}{\sqrt{1.5\cdot 1.5}}=\frac{2}{3}>0,
\]
which confirms the positive correlation.
\\
\\
3. \(T = 60 + 0.6W + 0.2S + \varepsilon,\) where \(\varepsilon \sim \mathcal{N}(0,5)\) is independent.
\[
\mathbb{E}[T] = 60 + 0.6\,\mathbb{E}[W] + 0.2\,\mathbb{E}[S] + \mathbb{E}[\varepsilon]
= 60 + 0.6(4) + 0.2(120) + 0
= 86.4.
\]
\[
\mathrm{Var}(T)=\mathrm{Var}(0.6W+0.2S)+\mathrm{Var}(\varepsilon)
\]
\[
=0.6^2\mathrm{Var}(W)+0.2^2\mathrm{Var}(S)+2(0.6)(0.2)\mathrm{Cov}(W,S)+5.
\]
\[
=0.36(1.5)+0.04(1.5)+2(0.12)(1)+5
=0.54+0.06+0.24+5
=5.84.
\]
\\
\\
4. Let a package be classified as fragile if \(W<4\).  For each package,
\[
p = \mathbb{P}(W<4).
\]
Since packages are iid, conditional on \(N=n\), the number of fragile packages follows a binomial distribution:
\[
F \mid N=n \sim \mathrm{Binomial}(n,p).
\]
The PMF is
\[
\mathbb{P}(F=k \mid N=n)=\binom{n}{k}p^k(1-p)^{n-k}, 
\quad k=0,1,\ldots,n,
\]
where
\[
p=\mathbb{P}(W<4).
\]
\\
\\
5. (a). \textbf{Code:}
\begin{verbatim}
mu = np.array([120, 4])
Sigma = np.array([[1.5, 1], [1, 1.5]])

rv_SW = mvn(mean=mu, cov=Sigma)

def simulate_T_star():
    N = poisson(mu=3).rvs(size=24).sum()
    
    if N == 0:
        return 0.0
    
    samples = rv_SW.rvs(size=N)
    S = samples[:, 0]
    W = samples[:, 1]
    
    eps = norm(loc=0, scale=np.sqrt(5)).rvs(size=N)
    
    T = 60 + 0.6 * W + 0.2 * S + eps
    
    return T.sum()
\end{verbatim}
\\
\\
5. (b). \textbf{Code:}
\begin{verbatim}
M = 1000
Tstar_samples = np.array([simulate_T_star() for _ in range(M)])

print("Empirical mean of T*:", Tstar_samples.mean())
print("Empirical std of T*:", Tstar_samples.std(ddof=1))
\end{verbatim}
\textbf{Output:}
\begin{verbatim}
Empirical mean of T*: 6203.037291870622
Empirical std of T*: 761.9121823483968
\end{verbatim}

\end{solution} 

\newpage 

\begin{problem} [Implementing a Linear Regression - Coding Review]
In this class, we will bridge theory and practice through implementing the methods that we cover from scratch. In this problem, we follow up on Problem 1 through exploring a more practical version of linear regression (fitting a linear model). Namely, we use ordinary least squares (OLS) to estimate a \textit{line of best fit} rather than a perfect fit to our data. Note that the focus of this problem is on coding rather than math---we will cover the relevant theory in much more depth during the course.\\

\noindent \textbf{Learning Goals:} In this problem, you will gain experience with the procedure of modeling real-world data. You will also get useful practice with debugging and writing clean, efficient code in Python. \\

\noindent Steve is a fictional CS 1810 TF giving a live demo of how to fit a linear regression. However, he quickly realizes that coding live in front of an audience isn't for the faint of heart. As a star student, you will help him with his code. Just like Problem 1, the demo uses a 2-D dataset, so that the goal is to model the relationship between the $x$ and $y$ coordinates. The data are stored in the \texttt{data} variable, with the first column corresponding to the $x$-coordinate and the second corresponding to the $y$-coordinate. 

\begin{enumerate}
    \item Using the provided data, Steve has defined variables $\texttt{y}$ and $\texttt{x}$ corresponding to the respective coordinates.  What is wrong with his current code? Fix the code and then plot the data. Does there appear to be a linear trend?

    \item Steve then defines a new variable \texttt{X}, which is meant to resemble $\bm X$ from Problem 1. Specifically, \texttt{X} is supposed to have one column of all ones (recall that this allows us to fit an intercept) and one column which is just $\texttt{x}$, the $x$-coordinates. However, he realizes that his code yields the wrong shape for $\texttt{X}$. What's going on here? Fix the code and then report what \texttt{y.shape} and \texttt{X.shape} are. Why is there no second coordinate in the output for \texttt{y.shape}?
    
    Hint: check the documentation for \texttt{np.hstack}. 

    \item Steve takes a much-needed break from coding to give the following high level overview of linear regression: given a target (response) $\bm y$ and features (predictors) $\bm X$, the goal of linear regression is to find weights $\bm w$ such that $\bm{\hat y} = \bm X \bm w$ closely approximates the true data $\bm y$. In OLS, we estimate $\bm w$ to be
    \[\bm{\hat w} = (\bm X^\top \bm X)^{-1}\bm X^\top \bm y\]
    Steve skips over the derivation of the result but assures you that you will learn it later in the course. What should the shape of $\bm{\hat w}$ be in Steve's demo?

    \item Having walked through the idea of linear regression, Steve then attempts to implement a 
    
    $\texttt{LinearRegression}$ class. He correctly identifies that we need 3 components: a constructor, a \texttt{fit} function for computing $\hat{\bm w}$ from the data, and a \texttt{predict} function for computing the estimate $\bm X \hat{\bm w}$. However, he realizes that there is something wrong (meaning logic or syntax) with at least one of these components. Please point out the issues, fix them, and include the plot of the fitted line.

    \item As his final act for the day, Steve introduces the Mean Squared Error (MSE) loss function:
    \[\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat y_i)^2\]
    This captures how well the outputs of our model, $\bm{\hat y}$, fit the actual data $\bm y$. Steve manages to correctly implement an MSE computation! However, you realize that he can vectorize his code to make it faster, meaning that he can directly compute the MSE from NumPy arrays without using any for loops. Implement the vectorized MSE and write down the corresponding mathematical expression, which should directly be in terms of the vectors $\bm y$ and $\hat{\bm y}$ rather than their components.
    
\end{enumerate}

\end{problem}

\begin{solution}

1. Fixed Code and Plot:
\begin{verbatim}
x = data[:, 0]
y = data[:, 1]

plt.scatter(x, y)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Scatter plot of data")
\end{verbatim}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig 4.1.png}
\end{figure}
Steve’s code incorrectly indexed rows instead of columns when extracting the variables \(x\) and \(y\). After fixing the code and plotting the data, the scatter plot shows a clear increasing linear trend. Thus, there appears to be an approximately linear relationship between \(x\) and \(y\), making a linear regression model appropriate.
\\
\\
2. Fixed Code:
\begin{verbatim}
ccintercept = np.ones((x.shape[0], 1))
x_col = x.reshape(-1, 1)
X = np.hstack([intercept, x_col])

print("y shape:", y.shape)
print("X shape:", X.shape)
\end{verbatim}
After correction, the shapes are:
\[
y.\text{shape} = (n,), \qquad X.\text{shape} = (n,2).
\]
There is no second coordinate in \texttt{y.shape} because \(y\) is stored as a one-dimensional array representing a vector of length \(n\), rather than as a two-dimensional column matrix.
\\
\\
3. Since the design matrix has shape \(X \in \mathbb{R}^{n \times 2}\) (one column for the intercept and one for \(x\)), the weight vector must satisfy
\[
\hat{y} = Xw \in \mathbb{R}^n.
\]
Therefore,
\[
w \in \mathbb{R}^{2 \times 1}.
\]
Equivalently, the OLS estimator
\[
\hat{w} = (X^\top X)^{-1}X^\top y
\]
has shape
\[
\hat{w} \in \mathbb{R}^{2\times 1},
\]
corresponding to the intercept and slope coefficients.
\\
\\
4. Fixed Code and Plot:
\begin{verbatim}
class LinearRegression:
    def __init__(self):
        self.w = None

    def fit(self, X, y):
        # Closed-form OLS solution
        self.w = np.linalg.inv(X.T @ X) @ X.T @ y
        return self.w

    def predict(self, X):
        return X @ self.w
\end{verbatim}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig 4.4.png}
\end{figure}


Steve’s implementation contained several issues:
\begin{itemize}
\item Matrix multiplication was incorrectly performed using elementwise \texttt{*} instead of \texttt{@}.
\item The matrix inverse was incorrectly written as \texttt{**-1} instead of using \(\left(X^\top X\right)^{-1}\).
\item The fitted weights were not stored in \texttt{self.w}.
\item The \texttt{predict} method did not reference \texttt{self.w} and was missing the \texttt{self} argument.
\end{itemize}
\\
\\
5. Revised Code: 
\begin{verbatim}
mse = np.mean((y - y_pred)**2)
print(mse)
\end{verbatim}
The Mean Squared Error can be written in vector form as
\[
\mathrm{MSE} = \frac{1}{n}\|y - \hat{y}\|_2^2
= \frac{1}{n}(y - \hat{y})^\top (y - \hat{y}).
\]
This computes the loss directly using vector operations without explicit loops.


\end{solution}

\end{document}
